集成学习分类：
欲得到泛化能力强的集成，集成中的基学习器应尽可能相互独立。虽然显示中很难“独立”，
但是可以设法使得基学习器尽可能具有较大的差异。
例如：对训练样本进行采样，产生若干个不同的子集，再从每个子集中训练出基学习器。
集成学习对应不同问题的解决方法：
（1）分类问题：使用投票法(计算基学习器分类结果最多的)
（2）回归问题：使用平均法()
1、boosting(代表：adaboost)：
    优点：泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整。
    缺点：对离散点敏感
    使用数据类型：数值型和标称型数据
    方法：使用对基分类器的线性组合来实现集成学习。权重计算公式：
        αt=1/2*ln(1-et/et)  # et表示错误率。
    标准adaboost只适用于二分类任务
2、bagging和随机森林：
计算的复杂度为：T(O(m)+O(s)) #O(m)是基学习器的复杂度 O(s)是采样与投票/平均过程的复杂度
(1)bagging：随机采样（用于为adaboost提供训练子集）
对m个样本进行m次采样，每次采一个，形成含有m个样本的采样子集，作为训练集中的一个集合。
根据e指数极限，约有63.2%的样本出现在采样集中。
bagging可以不经修改的用于多分类、回归等任务。

(2)随机森林RF：是bagging的扩展变体。
设共有d个属性，基决策树的每个节点一般随机选择k=math.log(d, 2)个属性，形成一个子集，
并对这个子集选择一个最优属性用于划分节点。
在决策树的训练过程中引入了随机属性选择。
RF通常会收敛到更低的泛化误差。
RF的训练效率常优于bagging，是因为在个体决策树构建过程中，bagging使用的是“确定型”决策树，
在选择划分属性的时候要节点的所有属性进行考察；而RF使用的“随机型”决策树，只需考察一个属性子集。

3、结合策略：
（1）简单平均法：数值型结果预测
（2）加权平均法
（3）投票法：分类问题最常用的策略。
    1）绝对多数投票法：选择票数过半的那个标签。
    2）相对多数投票法：选择票数最多的那个标签。
    3）加权投票法：与加权平均法类似。
h(x)基学习器的值类型：
（1）类标记：h(x)∈{0, 1}
（2）类概率：h(x)∈[0, 1]，相当于对后验概率P(cj|x)的估计。
<注意>：不同类型的h(x)基学习器不能混用。
